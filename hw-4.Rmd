---
title: "DSCI445 - Homework 4"
author: "Henrique Magalhaes Rio"
date: "Due 10/23/2019 by 4pm"
output: pdf_document
---

Be sure to `set.seed(445)` at the beginning of your homework. 

```{r}
#reproducibility
set.seed(445)

library(leaps)
library(ISLR)
library(tidyverse) 
library(knitr)
library(glmnet) # package for lasso and ridge regression
library(pls)
library(ISLR)
library(ggpubr)
library(MASS)
```

1. In this exercise, we will generate simulated data, and then use this data to perform best subset selection.

    a) Use `rnorm` to generate a predictor $X$ of length $n = 100$ and a noise vector $\epsilon$ also f length $n = 100$.
    
    
```{r}
X<- rnorm(100)
e<- rnorm(100,mean=0,sd=1.25)
f<- rnorm(100,mean=1,sd=1.25)
```
    
    
    b) Generate a response vector $Y$ of length $n = 100$ according to the model
        $$
        Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon
        $$
        where $\beta_0 = 1, \beta_1 = -0.5, \beta_2 = 2, \beta_3 = -1$.
        
        
```{r}
Y <- 1 + -0.5*X + 2*X^2 + -X^3 + e #fa
```
        
    
    c) Use the `regsubsets` function in the `leap` package to perform best subset selection in order to choose the best model containing the predictors $X, X^2, \dots, X^{10}$. What is the best model obtained according to $C_p$, BIC, and Adjusted $R^2$? Show some plots to provide evidence for your answer and report the coefficients of the best model obtained. 
    
        [**Hint 1:** The `poly` function may be useful for creating the model formula.]
        
        [**Hint 2:** You will need to make a data frame with your X and Y variables.]
        


```{r}

df<-data.frame(Y,X)

s<-regsubsets(x=poly(X,degree=10),y=Y,data=df,intercept=TRUE,nvmax = 10)
summ<-summary(s)
summ

summt <- data.frame(n=1:10,CP=summ$cp,BIC=summ$bic,AdjustedR2=summ$adjr2)


summt

coef(s,id=3)
coef(s,id=4)
ggarrange(
ggplot(summt)+geom_line(aes(x=n,y=CP,))+scale_x_continuous(breaks = 1:8),
ggplot(summt)+geom_line(aes(x=n,y=BIC))+scale_x_continuous(breaks = 1:8),
ggplot(summt)+geom_line(aes(x=n,y=AdjustedR2))+scale_x_continuous(breaks = 1:8))

```

Using CP and Bic we would use the model that include $x$, $x^2$ and $x^3$, but using adjusted R-Squared, we would use the model that includes $x$, $x^2$,$x^3$ and $x^8$, but its only slightly higher than the model with 3 variables that excludes $x^8$


        
    d) Repeat c. using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in c.?
    
```{r}
#forward
df<-data.frame(Y,X)

s<-regsubsets(x=poly(X,degree=10),y=Y,data=df,intercept=TRUE,method = "forward",nvmax = 10)
summ<-summary(s)
summ

summt <- data.frame(summ$cp,summ$bic,summ$adjr2)


summt

```

    
    
    
```{r}

#backward
df<-data.frame(Y,X)

s<-regsubsets(x=poly(X,degree=10),y=Y,data=df,intercept=TRUE,method = "backward",nvmax = 10)
summ<-summary(s)
summ

summt <- data.frame(summ$cp,summ$bic,summ$adjr2)


summt

```
    
The results are identical to the method used on c.    
    
    
    e) Now fit a lasso model to the simulated data using $X, X^2, \dots, X^{10}$ as predictors. Use $10$-fold CV to choose the optimal value of $\lambda$. Create plots of the CV error as a function of \lambda. Report the resulting coefficient estimates and discuss the results obtained.
    
```{r}
lambda <- 10^seq(-2, 10, length.out = 100)

cv_lasso <- cv.glmnet(poly(X,degree=10), Y, alpha = 1, lambda = lambda,standardize=TRUE)
ggplot() +
  geom_line(aes(cv_lasso$lambda, cv_lasso$cvm)) +
  geom_point(aes(cv_lasso$lambda, cv_lasso$cvm)) +
  scale_x_log10(labels = scales::comma)


lasso <- glmnet(poly(X,degree=10), Y, alpha = 1, lambda = cv_lasso$lambda.min,standardize = TRUE)

lasso$beta



```

 **It seems that the coefficients of LASSO model are quite similar to the subset selection, and both methods choose the same combination of Xs**


    
    
2. In this exercise we will predict the number of applications received using the other variables in the `College` data set (in the `ISLR` package).

    a) Split the data into training (60%) and "test" (40%) set randomly.

```{r}
n <- nrow(College)
trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n))
train <- College[trn,]
test<- College[!trn,]

```
    
    
    
    
    b) Fit a linear model using least squares on the training set and report the test set error obtained.

```{r}

lm <- lm(Apps~.,data=train)

pred<-predict(lm,test)
true <-test$Apps

se <- (true-pred)^2

mean(se)

```
    
    
    c) Fit a ridge regression model on the training set with $\lambda$ chosen using 10-fold CV (on the training set only). Report the test error obtained.
    

    
```{r}

lambda <- 10^seq(-10, 5, length.out = 100)
XC<-data.matrix(train[-2])
cv_ridgec <- cv.glmnet(XC, train$Apps, alpha = 0, lambda = lambda)
ridgec <- glmnet(XC, train$Apps, alpha = 0, lambda = cv_ridgec$lambda.min)

predr <- predict(ridgec,data.matrix(test[-2]))

ser <- (true-predr)^2
mean(ser)

```

    
    d) Fit the lasso on the training set with $\lambda$ chosen using 10-fold CV (on the training set only). Report the test error obtained, along with the number of non-zero coefficient estimates.
    
```{r}
cv_lassoc <- cv.glmnet(XC, train$Apps, alpha = 1, lambda = lambda)

lassoc <- glmnet(XC, train$Apps, alpha = 1, lambda = cv_lassoc$lambda.min)

predl <- predict(lassoc,data.matrix(test[-2]))

sel <- (true-predl)^2
mean(sel)


paste("Non-zero coefs :",cv_lassoc$nzero[which.min(cv_lassoc$cvm)])
```
    
    
    e) Fit a PCR model on the training set with $M$ chosen using 10-fold CV (on the training set only). Report the test error obtained, along with the value of $M$ selected by CV.
    
```{r}
pcr <- pcr(Apps ~ ., data = train, scale = TRUE, validation = "CV", val.type= "MSEP")

summary(pcr)

predpcr <- predict(pcr, test, ncomp = 17)


paste("test error", mean((true-predpcr)^2))

```
    
    
    f) Fit a PLS model on the training set with $M$ chosen using 10-fold CV (on the training set only). Report the test error obtained, along with the value of $M$ selected by CV.
    
    
    ```{r}
pls <- plsr(Apps ~ ., data = train, scale = TRUE, validation = "CV", val.type= "MSEP")

summary(pls)

predpls <- predict(pls, test, ncomp = 17)


paste("test error", mean((true-predpls)^2))

```
    
    
    g) Comment on the results obtained. How acurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?
    
  **None of them seem be very accurate in predicting the number of college applications as all of them have high test MSE values, the models seem to be pretty close in terms of test errors for the five approaches since they all suffer from having very large test errors.**
    
3. We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will explore this with a simulated data set. Run the following code to generate your dataset.

    ```{r}
    p <- 20
    n <- 1000
    
    X <- matrix(rnorm(n * p), nrow = n, ncol = p) ## predictors
    beta <- matrix(c(rnorm(8), rep(0, p - 8)), ncol = 1) ## 12 elements are equal to zero
    
    y <- X %*% beta + rnorm(n, 0, .5) ## y = Xbeta + epsilon
    ```

    a. Split your data into a training set containing 100 observations and a test set containting 900 observations.
    
    
```{r}

trn <- seq_len(n) %in% sample(seq_len(n), round(0.1*n))
trainx <- X[trn,]
trainy <- y[trn,]
testx <- X[!trn,]
testy <- y[!trn,]

train1<-data.frame(trainx,y=trainy)
test1<-data.frame(testx,y=testy)

```
    
    
    b. Perform best subset selection on the training set and plot the training set MSE associated with the best model of each size (**Hint** How do we calculate MSE?).
```{r}

subsel<-regsubsets(y~.,data=train1,intercept=FALSE,nvmax = 20)
trainm = model.matrix (y~., data = train1)






MSE <- rep(NA,8)
for(i in 1:20){
  
  
  coef <- coef(subsel,id=i)
  pred<-trainm[,names(coef)]*coef
  MSE[i]<-mean((train1$y-pred)^2)
  
  
}
MSE

msetdf<- data.frame(n=1:20,MSE)
ggplot(msetdf,aes(x=n,y=MSE))+geom_point()+geom_line()+scale_x_continuous(breaks = 1:20)

```
    
    
    c. Plot the test set MSE associated with the best model of each size. Hint, to get the predicted values, use matrix multiplication (`%*%`) to multiply the test data by the fitted coefficients of each model: $X\hat{\beta}$. Note there is no `predict` function for `regsubsets` objects.
```{r}
subsel<-regsubsets(y~.,data=train1,intercept=FALSE,nvmax = 20)
testm = model.matrix (y~., data = test1)

MSET <- rep(NA,20)
for(i in 2:20){
  
  
  coef <- coef(subsel,id=i)
  pred<-testm[,names(coef)]%*%coef
  MSET[i]<-mean((test1$y-pred)^2)
  
  
}

coef <- coef(subsel,id=1)
  pred<-testm[,names(coef)]*coef
  MSET[1]<-mean((test1$y-pred)^2)
MSET
msetdf<- data.frame(n=1:20,MSET)
ggplot(msetdf,aes(x=n,y=MSET))+geom_point()+geom_line()+scale_x_continuous(breaks = 1:20)
```
    
    
    d. For which model size does the test set MSE take its minimum value? Comment on your results.
    
  **The test MSE had the smallest value at the model with 8 predictors, while in regards with the training data set the model with lowest MSE is the one with one predictor**
    
    
    e. How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.
```{r}
data.frame(coef=coef(subsel,id=8),beta=beta[1:8])
```
   **The model that minimizes test MSE seems to be pretty close to the betas of the true model for most of the Xs ** 
    
4. We will now try to predict per capita crime rate in the `Boston` data set from the `ISLR` package.

    a. Try out some of the regression methods explored in this chapter such as best subset, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.
    
```{r}

  
  
n <- nrow(Boston)
trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n))
btrain <- Boston[trn,]
btest<- Boston[!trn,]

XB <- data.matrix(btrain[-1])

trueb <- btest$crim

```
    
    
    
```{r}

set.seed(445)
cv_lassob <- cv.glmnet(XB, btrain$crim, alpha = 1, lambda = lambda)

ggplot() +
  geom_line(aes(cv_lassob$lambda, cv_lassob$cvm)) +
  geom_point(aes(cv_lassob$lambda, cv_lassob$cvm)) +
  scale_x_log10(labels = scales::comma)


lassob <- glmnet(XB, btrain$crim, alpha = 1, lambda = cv_lassob$lambda.min)
lassob$beta
predl <- predict(lassob,data.matrix(btest[-1]))

sel <- (trueb-predl)^2
paste("Test MSE",mean(sel))
paste("Mim Lambda", cv_lassob$lambda.min)





```
    

    
    
```{r}
set.seed(445)
cv_ridgeb <- cv.glmnet(XB, btrain$crim, alpha = 0, lambda = lambda)

ggplot() +
  geom_line(aes(cv_ridgeb$lambda, cv_ridgeb$cvm)) +
  geom_point(aes(cv_ridgeb$lambda, cv_ridgeb$cvm)) +
  scale_x_log10(labels = scales::comma)


ridgeb <- glmnet(XB, btrain$crim, alpha = 0, lambda = cv_ridgeb$lambda.min)
ridgeb$beta


predr <- predict(ridgeb,data.matrix(btest[-1]))

sel <- (trueb-predr)^2
paste("Test MSE",mean(sel))

```

    
    
```{r}
set.seed(445)
pcrb <- pcr(crim ~ ., data = btrain, scale = TRUE, validation = "CV", val.type= "MSEP")

summary(pcrb)

predpcrb <- predict(pcrb, btest, ncomp = 13)


paste("test error", mean((trueb-predpcrb)^2))
```

 
  
  
  **I choose to do ridge regression, PCR, and LASSO. By using the CV in order to tune the lambda it seems that the LASSO had used 10 predictors, and it seemed to be similar to the predictors that are closer to 0 in the ridge regression in which CV was used to tune the lambda. For PCR, after CV the M with the lowest error was 13.**
    
    b. Propose a model or a set of models that seem to perform well on this data set and justify your answer. Make sure that you are evaluating performance using validation error, cross-validation error, or some other reasonable alternative (not just training error).
    
    
  **The model I would choose is the LASSO with a lambda= 0.3511, since it performed the best in the validation data set with a test error of around 32.02, when compared to 32.03 of the ride model and 33.6 of the PCR model.**
    
    
    
    c. Does your chosen model involve all of the features in the data set? Why or why not?
    
  **In the LASSO model it only includes 10 of the 13 variables, chas,tax and age have their coefficients equal to zero.  **

Turn in in a pdf of your homework to canvas using the provided Rmd file as a template. Your Rmd file on rstudio.cloud will also be used in grading, so be sure they are identical.