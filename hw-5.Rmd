---
title: "DSCI445 - Homework 5"
author: "Henrique Magalhaes Rio"
date: "Due 11/06/2019 by 4pm"
output: pdf_document
---

Be sure to `set.seed(445)` at the beginning of your homework. 

```{r}
#reproducibility
set.seed(445)
library(leaps)
library(ISLR)
library(tidyverse) 
library(knitr)
library(glmnet) # package for lasso and ridge regression
library(pls)
library(ISLR)
library(ggpubr)
library(MASS)
library(splines)
library(gam)
library(tree)
library(randomForest)
library(gbm)
library(caret)
```

# Non-linear Models

1. We know that a cubic regression spline with one knot at $\xi$ can be obtained using a basis of the form $x, x^2, x^3, (x - \xi)^3_+$ where $(x - \xi)^3_+ = (x - \xi)^3$ if $x > \xi$ and $0$ otherwise. We will now show that a function of the form
$$
f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4(x - \xi)^3_+
$$
is a cubic regression spline, regardless of the values of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$.

    a) Find a cubic polynomial
        $$
        f_1(x) = a_1 + b_1 x + c_1 x^2 + d_1 x^3
        $$
        such that $f(x) = f_1(x)$ for all $x \le \xi$. Express $a_1, b_1, c_1, d_1$ in terms of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$.
        
        
        
        
    b) Find a cubic polynomial
        $$
        f_2(x) = a_2 + b_2 x + c_2 x^2 + d_2 x^3
        $$
        such that $f(x) = f_2(x)$ for all $x > \xi$. Express $a_2, b_2, c_2, d_2$ in terms of $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4$. We have now established that $f(x)$ is a piecewise polynomial.
        
![A/B](/cloud/project/PXL_20211108_215412284.jpg)
        
    c) Show that $f_1(\xi) = f_2(\xi)$. That is, $f(x)$ is continuous at $\xi$.
    
    d) Show that $f'_1(\xi) = f'_2(\xi)$. That is, $f'(x)$ is continuous at $\xi$.
    
![C/D](/cloud/project/PXL_20211108_220727753.jpg)
    
    e) Show that $f''_1(\xi) = f''_2(\xi)$. That is, $f''(x)$ is continuous at $\xi$.


![E](/cloud/project/PXL_20211108_220742739.jpg)
    
 


















    
2. In this exercise, we will further analyze the `Wage` data set.
    
    a) Perform polynomial regression to predict `wage` using `age`. Use cross-validation to select the optimal degree $d$ for the polynomial. What degree was chosen? Make a plot of the fit obtained.
```{r}

set.seed(445)
res <- 0
fit_poly <- list()
d <- 7 # highest degree to check
n <- nrow(Wage)

trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n))

for(i in seq_len(d)) {
    
    
    fit_poly[[i]] <- lm(bquote(wage ~ poly(age, .(i))), data = Wage[trn,])
    
    
    res[i] <- mean((Wage[!trn,]$wage-predict(fit_poly[[i]],Wage[!trn,]))^2)

    
}

    
    
testmse <- data.frame(degree=1:7,res)


testmse

ggplot(Wage, aes(x=age, y=wage))+
  geom_point() +
  stat_smooth(aes(y=wage), method = "lm", formula = y ~ x + I(x^2)+I(x^3)+I(x^4))  

```
    
        
    b) Use ANOVA (`anova()`) to compare your polynomial regressions from part a (Note: we can do this comparison because the models are nested). Which degree model would you choose? Compare this to your results from part a.
    
```{r}

set.seed(445)
do.call(anova, fit_poly)
```
    
        
    c) Fit a step function to predict `wage` using `age` and perform cross validation to choose the optimal number of cuts. Make a plot of the fit obtained.
    
```{r}
set.seed(445)
res <- 0
fit_cut <- list()
d <- 10 # highest degree to check
n <- nrow(Wage)

trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n))

age_grid <- data.frame(age = seq(min(Wage$age), max(Wage$age), length.out = 1000))

wage<-Wage

for(i in 2:10) {
    
  
    wage <- wage %>% mutate(agecut=cut(age,breaks=i))
  
    fit_cut[[i]] <- lm(wage~agecut, data = wage[trn,])
    

    res[i] <- mean((wage[!trn,]$wage-predict(fit_cut[[i]],wage[!trn,]))^2) 
    

    
}


data.frame(breaks=1:10,res)

step <- lm(wage ~ cut(age, breaks = 5), data = wage)




age_grid <- data.frame(age = seq(min(wage$age), max(wage$age), length.out = 1000))

ggplot() +
  geom_point(aes(age, wage), data = wage,alpha=0.5) +
  geom_line(aes(age_grid$age, predict(step, age_grid)), colour = "red") 


```
    
    
    
        
3. The `Wage` data set contains a number of other features not explored in this chapter, such as marital status (`maritl`) and job class (`jobclass`). Explore the relationships between some of these other predictors and `wage` and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained and write a summary of your findings.


```{r}

set.seed(445)
n <- nrow(Wage)
trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n))

## Choose predictors
trainw <- Wage[trn,]
testw <- Wage[!trn,]


gam <- gam(wage ~jobclass+ns(age,df=4)+ns(year,df=5), data = trainw)
gam2 <- gam(wage ~education+jobclass+ns(age,df=7)+ns(year,df=3)+health_ins, data = trainw)
gam3 <- gam(wage ~race+education+jobclass+ns(age,df=7)+ns(year,df=3)+health_ins, data = trainw)


pred <- predict(gam,testw)
pred2 <- predict(gam2,testw)
pred3 <- predict(gam3,testw)


testerror <- mean((pred-testw$wage)^2)
testerror
testerror2<- mean((pred2-testw$wage)^2)
testerror2
testerror3<- mean((pred3-testw$wage)^2)
testerror3

plot(gam3)

gam3
```
 
 **I choose to use the categorical variables race,education,education, and jobclass and the continuous variables age and year, with age being a natural spline with 7 degrees of freedom and year a natural spline with 3 degrees of freedom. It also seems that there is evidence of non-linearity between wage and age/year.**
 
     
4. This question relates to the `College` data set. 
    
    a) Split the data into a training (60%) and test data set (40%). Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses a subset of the predictors.
    
```{r}

n <- nrow(College)
trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n))

train <- College[trn,]
test <- College[!trn,]


s<-regsubsets(Outstate~.,data=train,intercept=TRUE,method= "forward",nvmax = 18)

summ<-summary(s)


summt <- data.frame(n=1:17,CP=summ$cp,BIC=summ$bic,AdjustedR2=summ$adjr2)


summt

best <-coef(s,13)[-1]
names(best)
```
    
        
    b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in part a. as predictors with natural cubic splines and df = 6 for each continous variable. Plot the results and explain your findings.
    
    

```{r}

train <- train[,c("Outstate","Private","Apps","Accept","Enroll","Top10perc","F.Undergrad","Room.Board","Personal","PhD","Terminal","perc.alumni","Expend","Grad.Rate")]

fit_gam <- gam(Outstate ~ Private+ns(Apps,df=6)+ns(Accept,df=6)+ns(Enroll,df=6)+ns(Top10perc,df=6)+ns(F.Undergrad,df=6)+ns(Room.Board,df=6)+ns(Personal,df=6)+ns(PhD,df=6)+ns(Terminal,df=6)+ns(perc.alumni,df=6)+ns(Expend,df=6)+ns(Grad.Rate,df=6), data = train)

plot(fit_gam,se=TRUE)

```
    
        
    c) Evaluate the model obtained on the test set and explain the results obtained.


    
```{r}
set.seed(445)
test <- test[,c("Outstate","Private","Apps","Accept","Enroll","Top10perc","F.Undergrad","Room.Board","Personal","PhD","Terminal","perc.alumni","Expend","Grad.Rate")]
pred <- predict(fit_gam,test)

testerror <- mean((pred-test$Outstate)^2)
testerror

summary(fit_gam)

```


**There seems to be a high test error rate, and one variable not statistically significant**
        
    d) For which variables is there evidence of a non-linear relationship with the response?
    
**For all variables  there seems to be evidence of a non-linear relationship**
        
5. GAMs are generally fit using a *back-fitting* approach. The idea behind back fitting is quite simple and we will use a linear regression example to explore it.
    
    Suppose we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. We could take the following iterative approach:
        
        i. Hold all but one coefficient estimate fixed at it's current value.
        ii. Update only the one coefficient using simple linear regression.
        iii. Move through and update all coefficients using steps i.-ii.
            
    Repeat the above approach until we have reached *convergence* -- that is, until the coefficient esttimates stop changing. We will try this on a toy example.
        
    a. Generate a response $Y$ and two predictors $X_1$ and $X_2$ using the following model:
        $$
        Y = 1 + 3X_1 - 4X_4 + \epsilon, \epsilon \sim N(0, 0.5)
        $$
        where $X_1, X_2 \sim N(0, 1)$.
```{r}

x1 <- rnorm(1000)
x2 <- rnorm(1000)
e<- rnorm(1000,0,0.5)

y <- 1 +3*x1+4*x2+e

```
        
    b. Initialize $\hat{\beta}_1$ to take value $= 10$.
```{r}
b1h <-10
```
    
    
    c. Keeping $\hat{\beta}_1$ fixed, fit the model
    
        $$
        Y - \hat{\beta}_1 X_1 = \beta_0 + \beta_2 X_2 + \epsilon
        $$
        Set $\hat{\beta}_2 = $ the resulting coefficient from your fit.
        
```{r}
slr1 <- lm(y-b1h*x1~x2)
summary(slr1)

b2h<-summary(slr1)$coefficients[2]
```
        
    
    d. Keeping $\hat{\beta}_2$ fixed, fit the model
    
        $$
        Y - \hat{\beta}_2 X_2 = \beta_0 + \beta_1 X_1 + \epsilon
        $$
        Set $\hat{\beta}_1 = $ the resulting coefficient from your fit.
        
```{r}
slr2 <- lm(y-b2h*x2~x1)
summary(slr2)

b1h<-summary(slr2)$coefficients[2] 



```
        
        
        
        
    e. Write a for loop to repeat c. and d. 1000 times. Make a line plot of the estimates of $\hat{\beta}_0, \hat{\beta_1}, \hat{\beta}_2$, at each iteration of the for loop with $\hat{\beta}_0, \hat{\beta_1}, \hat{\beta}_2$ each in a different color.
    
```{r}
slr <- list()
b1h<-10

store <- matrix(ncol = 3,nrow = 1000)


for(i in 1:1000) {
    
  
    lm1 <- lm(y-b1h*x1~x2)

    b2h<-summary(lm1)$coefficients[2]
    store[i,3]<-b2h
    

    lm2 <- lm(y-b2h*x2~x1)

    b1h<-summary(lm2)$coefficients[2]
    b0h<-summary(lm2)$coefficients[1]
    store[i,2]<-b1h
    store[i,1]<-b0h
}


i <-1:1000

ggplot()+ geom_line(aes(i, store[,1]),color="red")+ geom_line(aes(i, store[,2]),color="cyan")+ geom_line(aes(i, store[,3]))
```
    
    
    f. Compare your answer to e. to the results of performing multiple linear regression to predict $Y$ using $X_1$ and $X_2$ To do this, overlay a horizontal line for each coefficient value on your plot from e. (You can use `ggplot::geom_hline()` to add the horizontal lines).
    
```{r}
lm <-lm(y~x1+x2)

summary(lm)

ggplot()+ geom_line(aes(i, store[,1]),color="red")+ geom_line(aes(i, store[,2]),color="cyan")+ geom_line(aes(i, store[,3]))+geom_hline(aes(yintercept= 1.00819),color='grey86')
```
    
    
    g. On this data set, how many backfitting iterations were required to obtain a "good" approximation to the multiple regression coefficients.
    
**It seems it only took 2 iterations in order for the algorithm to converge on the values of the coefficients.**
    
    h. Choose a different starting value for $\beta_1$ and repeat steps b.-g. Compare your results.
```{r}
slr <- list()
b1h<-100

store <- matrix(ncol = 3,nrow = 1000)


for(i in 1:1000) {
    
  
    slr1 <- lm(y-b1h*x1~x2)
    summary(slr1)

    b2h<-summary(slr1)$coefficients[2]
    store[i,3]<-b2h
    

    slr2 <- lm(y-b2h*x2~x1)
    summary(slr2)

    b1h<-summary(slr2)$coefficients[2]
    b0h<-summary(slr2)$coefficients[1]
    store[i,2]<-b1h
    store[i,1]<-b0h
}


i <-1:1000

ggplot()+ geom_line(aes(i, store[,1]),color="red")+ geom_line(aes(i, store[,2]),color="cyan")+ geom_line(aes(i, store[,3]))
```
    


# Tree-based Models

6. This problem involves the $OJ$ data set in the $ISLR$ package.

    a. Create a training set containing a random sample of $800$ observations and a test set containing the remaining observations.


```{r}
set.seed(445)
n <- nrow(OJ)
trn <- seq_len(n) %in% sample(seq_len(n), round(800/1070*n))

trainoj <- OJ[trn,]
testoj <-OJ[!trn,]



```
  
    
    b. Fit a tree to the training data with `Purchase` as the response and the other variables as predictors. Use the `summary()` function to produce summary information about the tryy and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
    
    
```{r}
set.seed(445)

tree <- tree(Purchase~.,data= trainoj)
  summary(tree)
```

***It seems that the tree has 7 terminal nodes with low training error rate of 83.25%**
    
    c. Type in the name of the tree object to get a detailed text output. Pick one of the terminal nodes and interpret the information displayed.
```{r}
tree
```
    The first terminal node is LoyalCH which splits the tree at around 0.5.
    
    d. Create a plot of the tree and interpret.
```{r}
plot(tree)
text(tree)
```
    **LoyalCH seems to be the most important factor in determining the choice of Juice brand, while PriceDiff comes second and PriceMM third.**
    
    e. Predict the response on the test data and produce a confusion matric comparing the test labels to the predicted labels. What is the test error rate?
```{r}
set.seed(445)

treet = predict(tree, testoj, type = "class")
table(predicted = treet, actual = testoj$Purchase)
```
    The test error rate is 85.19%
    
    f. Apply the `cv.tree()` function to the training set in order to determine the optimal tree size.
    
```{r}

set.seed(445)
k=1:ncol(OJ)


cvtree <- cv.tree(tree, FUN=prune.misclass)

index <- which.min(cvtree$dev)

cvtree$size[index]



```
    
    
    g. Produce a plot with tree size on the $x$-axis and CV error rate on the $y$-axis. Which tree size corresponds to the lowest CV classification error rate?
```{r}
ggplot()+geom_line(aes(x=cvtree$size,cvtree$dev))
```


**A Tree of size 7 would be the lowest classicfication error rate.**
    
    h. Produce a pruned tree corresponding to the optimal tree size. If CV doesn't lead to the selection of a pruned tree, then create a pruned tree with five terminal nodes.
    
```{r}
prunetree <- prune.misclass(tree, best = 5)
prunetree
```
    
    
    i. Compare the training error rates between the pruned and unpruned tree.
```{r}
summary(prunetree)
summary(tree)
```
    
    
**The Pruned tree seems to have a higher error rate than the Unpruned tree**
    
    
    j. Compare the test error rates between the pruned and unpruned tree.
    
```{r}
treet = predict(tree, testoj, type = "class")
table(predicted = treet, actual = testoj$Purchase)

treetp = predict(prunetree, testoj, type = "class")
table(predicted = treetp, actual = testoj$Purchase)
```
**Test Error rate of regular tree is 81% and for the pruned tree is 78%**

7. We will use boosting, bagging, and random forests to predict `Salary` in the `Hitters` data set.

    a. Remove the observations for which the salary information is unknown and then log-transform the salaries.


```{r}
df <- Hitters

df <- na.omit(df)

df <- df %>% mutate(Salary = log(Salary))



```
    
    
    b. Create a training set consisting of the first $200$ observattions and a test set consisting of the remaining observations.
```{r}


```
    
    
    c. Perform boosting on the training set with $1,000$ trees for a range of values of the shrinkage parameter $\lambda$. Produce a plot with different shrinkage values on the $x$-axis and the corresponing training MSE on the $y$-axis.
    
```{r}
lambda=seq(from=0,to=1,length.out=100)

tmse<-0

for(i in 1:100){
boost <- gbm(Salary~.,data=df[1:200,],n.trees=1000,shrinkage=lambda[i],interaction.depth = 2, distribution = "gaussian")


pr <- predict(boost,df[1:200,])

trainmse <- mean((pr-df[1:200,]$Salary)^2)
tmse[i]<-trainmse

}




```
```{r}
ggplot()+geom_line(aes(x=lambda,y=tmse))
```
    
    
    d. Produce a plot with different shrinkage values on the $x$-axis and the corresponing test MSE on the $y$-axis.
    
```{r message=FALSE, warning=FALSE, echo=FALSE}
set.seed(445)
lambda=seq(from=0,to=1,length.out=100)

tmse<-0

for(i in 1:100){
boost <- gbm(Salary~.,data=df[1:200,],n.trees=1000,shrinkage=lambda[i],interaction.depth = 2, distribution = "gaussian")


pr <- predict(boost,df[201:nrow(df),])

trainmse <- mean((pr-df[201:nrow(df),]$Salary)^2)
tmse[i]<-trainmse

}


lambda[which.min(tmse)]
```
    
    ```{r}
ggplot()+geom_line(aes(x=lambda,y=tmse))
    lambda[which.min(tmse)]

```
    e. Compare the test MSE of boosting to the test MSE that results from two other regression approaches (Something from Ch. 3, 6, or 7)
    
```{r}
set.seed(445)

true<-df[201:nrow(df),]$Salary

pcr <- pcr(Salary ~ ., data = df[1:200,], scale = TRUE, validation = "CV", val.type= "MSEP")

##summary(pcr)

predpcr <- predict(pcr, df[201:nrow(df),],ncomp = 6)


paste("test error PCR", mean((true-predpcr)^2))


boosted <- gbm(Salary~.,data=df[1:200,],n.trees=1000,shrinkage=0.05050505,interaction.depth = 2, distribution = "gaussian")


pr <- predict(boosted,df[201:nrow(df),])

testnmse <- mean((pr-true)^2)


paste("Boosted tree test error",testnmse)



knn <- knnreg(Salary~.,data=df[1:200,],k=2)

knnpr <- predict(knn,df[201:nrow(df),])

testkmse<-mean((knnpr-true)^2)

paste("Knn test error",testkmse)

```
    
    
    f. Which variables appear to be the most important predictors in the boosted model?
    
```{r}
summary(boosted)
```
    
**It seems that CRBI is the most important variable**
    
    g. Now apply bagging to the training data set. What is the test MSE for this approach?
    
    
```{r}
set.seed(445)
bag_fit <- randomForest(Salary ~ ., data = df[1:200,], mtry = ncol(df) - 1, importance = TRUE)

bag <- predict(bag_fit,df[201:nrow(df),])

testnmse <- mean((bag-true)^2)


paste("Bagged tree test error",testnmse)

```
    
    
    h. Now apply random forest to the training data set. What is the test MSE for this approach?
    
```{r}
rf_fit <- randomForest(Salary ~ ., data = df[1:200,], mtry = sqrt(ncol(df) - 1), importance = TRUE)


rf <- predict(rf_fit,df[201:nrow(df),])

testnmse <- mean((rf-true)^2)


paste("Random forest tree test error",testnmse)
```
    

Turn in in a pdf of your homework to canvas using the provided Rmd file as a template. Your Rmd file on rstudio.cloud will also be used in grading, so be sure they are identical.